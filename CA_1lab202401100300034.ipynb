{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2xx0a7Yr-xI",
        "outputId": "dfb28918-9b67-4960-e828-d40c9862997d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training shape: (50000, 32, 32, 3)\n",
            "Testing shape: (10000, 32, 32, 3)\n",
            "Epoch 1/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 140ms/step - accuracy: 0.2996 - loss: 1.8820 - val_accuracy: 0.4984 - val_loss: 1.3731\n",
            "Epoch 2/10\n",
            "\u001b[1m195/391\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 132ms/step - accuracy: 0.5097 - loss: 1.3585"
          ]
        }
      ],
      "source": [
        "# Import TensorFlow library\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import required modules from Keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "# Import matplotlib (used for plotting graphs if needed)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "# It contains 60,000 color images of size 32x32 in 10 classes\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "\n",
        "# Normalize pixel values from range (0-255) to (0-1)\n",
        "# This improves training performance\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "\n",
        "# Print dataset shapes\n",
        "print(\"Training shape:\", x_train.shape)   # (50000, 32, 32, 3)\n",
        "print(\"Testing shape:\", x_test.shape)     # (10000, 32, 32, 3)\n",
        "\n",
        "\n",
        "# Create Sequential model (layers added one after another)\n",
        "model = models.Sequential()\n",
        "\n",
        "\n",
        "# First Convolution Layer\n",
        "# 32 filters of size 3x3\n",
        "# ReLU activation\n",
        "# Input shape is 32x32 with 3 color channels (RGB)\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
        "\n",
        "# MaxPooling layer reduces spatial size by 2x2\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "\n",
        "# Second Convolution Layer\n",
        "# 64 filters\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "\n",
        "# MaxPooling again\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "\n",
        "# Third Convolution Layer\n",
        "# 64 filters\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "\n",
        "\n",
        "# Flatten 3D feature maps into 1D vector\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "\n",
        "# Fully connected dense layer\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "\n",
        "# Output layer\n",
        "# 10 neurons (for 10 classes)\n",
        "# No activation because we use logits\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',   # Adaptive learning optimizer\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,              # Train for 10 iterations\n",
        "    batch_size=128,         # Process 128 images at once\n",
        "    validation_data=(x_test, y_test)\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_acc)"
      ]
    }
  ]
}